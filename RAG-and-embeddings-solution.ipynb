{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìù *Solution* Task: Semantic Retrieval-Augmented Question Answering Using Groq LLM\n",
    "\n",
    "## Objective\n",
    "Implement a question-answering system that:\n",
    "1. Retrieves the most semantically relevant text passages to a user query.\n",
    "2. Constructs a natural language prompt based on the retrieved content.\n",
    "3. Uses a large language model (LLM) hosted by Groq to generate an answer.\n",
    "\n",
    "---\n",
    "\n",
    "## Task Breakdown\n",
    "\n",
    "### 1. Embedding-Based Semantic Retrieval\n",
    "- Use the `SentenceTransformer` model `\"Sahajtomar/German-semantic\"` to encode a user query into a dense vector embedding.\n",
    "- Perform a nearest-neighbor search in a prebuilt FAISS index to retrieve the top-**k** similar text chunks. You can **use the prebuilt FAISS form above**.\n",
    "\n",
    "\n",
    "### 2. LLM Prompt Construction and Query Answering\n",
    "- Build the prompt:\n",
    "  - Using the retrieved text chunks, concatenates the results into a context block.\n",
    "  - Builds a **prompt** asking the LLM to answer the question using that context.\n",
    "  - Sends the prompt to the **Groq LLM API** (`llama-3.3-70b-versatile`) and returns the response.\n",
    "\n",
    "### 3. User Query Execution\n",
    "- An example query (`\"What is the most important factor in diagnosing asthma?\"`) is used to demonstrate the pipeline.\n",
    "- The final answer from the LLM is printed.\n",
    "\n",
    "\n",
    "## Tools & Models Used\n",
    "- **SentenceTransformers** (`Sahajtomar/German-semantic`) for embedding generation.\n",
    "- **FAISS** for efficient vector similarity search.\n",
    "- **Groq LLM API** (`llama-3.3-70b-versatile`) for generating the final response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from dotenv import load_dotenv\n",
    "from groq import Groq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_api_key = \"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\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"Sahajtomar/German-semantic\")\n",
    "\n",
    "index = faiss.read_index(\"faiss/faiss_index.index\")\n",
    "with open(\"faiss/chunks_mapping.pkl\", \"rb\") as f:\n",
    "    token_split_texts = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_texts(query, k, index, token_split_texts, model):\n",
    "    query_embedding = model.encode([query], convert_to_numpy=True)\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "\n",
    "    # Filter out-of-bounds indices\n",
    "    valid_indices = [i for i in indices[0] if i < len(token_split_texts)]\n",
    "    retrieved_texts = [token_split_texts[i] for i in valid_indices]\n",
    "\n",
    "    return retrieved_texts, distances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_query(query, k, index, texts, groq_api_key):\n",
    "    retrieved_texts, _ = retrieve_texts(query, k, index, texts, model)\n",
    "    context = \"\\n\\n\".join(retrieved_texts)\n",
    "\n",
    "    prompt = (\n",
    "        \"Answer the following question using the provided context. \"\n",
    "        \"Explain it as if you are explaining it to a 5 year old.\\n\\n\"\n",
    "        f\"Context:\\n{context}\\n\\n\"\n",
    "        f\"Question: {query}\\nAnswer:\"\n",
    "    )\n",
    "\n",
    "    client = Groq(api_key=groq_api_key)\n",
    "    messages = [{\"role\": \"system\", \"content\": prompt}]\n",
    "    llm = client.chat.completions.create(\n",
    "        messages=messages,\n",
    "        model=\"llama-3.3-70b-versatile\"\n",
    "    )\n",
    "\n",
    "    answer = llm.choices[0].message.content\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of text chunks: 62\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of text chunks:\", len(token_split_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Answer:\n",
      " Oh boy, let's talk about asthma. So, you know how sometimes you might get a cough or it's hard to breathe? That can be because of something called asthma. \n",
      "\n",
      "To figure out if someone has asthma, doctors need to know some things. The most important thing is... (are you ready?) ...how the person is feeling and what happens to their body when they breathe. \n",
      "\n",
      "The doctor will ask questions like: \"Do you cough a lot?\" or \"Do you get tired when you run around?\" They might also want to know if you have any yucky feelings in your chest or if it's hard to breathe sometimes.\n",
      "\n",
      "But, the best way to know for sure is to do some special tests. These tests can help the doctor see how well your lungs are working. It's kind of like a special check-up for your breathing.\n",
      "\n",
      "So, the most important thing in diagnosing asthma is to listen to how the person is feeling and do those special tests to see how their lungs are working. That way, the doctor can say for sure if someone has asthma or not. And if they do, they can help them feel better. Isn't that cool?\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the most important factor in diagnosing asthma?\"\n",
    "answer = answer_query(query, k=5, index=index, texts=token_split_texts, groq_api_key=groq_api_key)\n",
    "print(\"LLM Answer:\\n\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Answer:\n",
      " Das ist eine gro√üartige Frage!\n",
      "\n",
      "Stell dir vor, es gibt eine Abteilung, die sich um Geld und Banken k√ºmmert. Das ist die Abteilung \"Banking & Finance\". \n",
      "\n",
      "In dieser Abteilung gibt es einige wichtige Themen, auf die man sich konzentriert. Das sind wie Spielthemen, auf die man sich fokussiert.\n",
      "\n",
      "Einige dieser Themen sind:\n",
      "\n",
      "* Wie man Geld anlegt, damit es mehr wird\n",
      "* Wie man sicherstellt, dass das Geld nicht verloren geht\n",
      "* Wie man mit anderen L√§ndern und Unternehmen Gesch√§fte macht\n",
      "* Wie man neue Ideen entwickelt, um das Geld besser zu nutzen\n",
      "\n",
      "Das sind ein bisschen wie R√§tsel, die man l√∂sen muss, um das Geld und die Banken zu verstehen. Die Leute in dieser Abteilung arbeiten daran, all diese Themen zu verstehen und zu verbessern.\n",
      "\n",
      "Ich hoffe, das hilft!\n"
     ]
    }
   ],
   "source": [
    "query = \"Welche Schwerpunktthemen hat es in der Abteilung Banking & Finance?\"\n",
    "answer = answer_query(query, k=5, index=index, texts=token_split_texts, groq_api_key=groq_api_key)\n",
    "print(\"LLM Answer:\\n\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Answer:\n",
      " Das ist ein bisschen komisch! Wir haben gerade √ºber etwas anderes gesprochen, n√§mlich √ºber Computer-Programme, die Texte schreiben k√∂nnen. Aber jetzt willst du wissen, wann in Bali die Regenzeit ist?\n",
      "\n",
      "Also, Bali ist eine Insel in Indonesien und sie hat eine Regenzeit, die normalerweise von November bis M√§rz dauert. Es regnet viel in dieser Zeit, aber es ist auch sch√∂n gr√ºn und die Insel ist sehr sch√∂n. Wenn du nach Bali reisen m√∂chtest, solltest du vielleicht besser in der Trockenzeit von April bis Oktober kommen, wenn die Sonne scheint und es nicht so viel regnet.\n",
      "\n",
      "Aber ich denke, wir sollten uns wieder auf die Computer-Programme konzentrieren, wenn du m√∂chtest! Oder willst du wirklich mehr √ºber Bali wissen?\n"
     ]
    }
   ],
   "source": [
    "query = \"Beschreibe mir die Regenseason von Bali\"\n",
    "answer = answer_query(query, k=5, index=index, texts=token_split_texts, groq_api_key=groq_api_key)\n",
    "print(\"LLM Answer:\\n\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Answer:\n",
      " So, you know how we can use computers to play games and watch videos? Well, there's something called Artificial Intelligence, or AI for short, that can help us learn new things.\n",
      "\n",
      "Imagine you have a special teacher that can help you with your homework, and it's like a super smart robot. This robot can show you pictures and videos to help you understand things better. It can even talk to you and answer your questions.\n",
      "\n",
      "Right now, AI can help us learn in many ways. It can:\n",
      "\n",
      "* Help us practice our reading and math skills with fun games and quizzes.\n",
      "* Show us videos and pictures to help us understand things like science and history.\n",
      "* Even help our real teachers make learning more fun and exciting.\n",
      "\n",
      "In the future, AI might be able to do even more cool things to help us learn. It might be able to:\n",
      "\n",
      "* Create special lessons just for us, based on what we like and what we're good at.\n",
      "* Help us learn new languages, like Spanish or Chinese.\n",
      "* Even help us explore virtual worlds and go on virtual field trips to places like the moon or the zoo.\n",
      "\n",
      "So, AI is like a special tool that can help us learn and have fun at the same time. And who knows, maybe one day we'll have our own personal robot teacher to help us learn and grow. Wouldn't that be awesome?\n"
     ]
    }
   ],
   "source": [
    "query = \"How can AI help with learning currently and in the future?\"\n",
    "answer = answer_query(query, k=5, index=index, texts=token_split_texts, groq_api_key=groq_api_key)\n",
    "print(\"LLM Answer:\\n\", answer)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
